{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.sparse as sp\n",
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from recbole.utils import InputType\n",
    "from recbole.model.abstract_recommender import GeneralRecommender\n",
    "from recbole.model.loss import BPRLoss, EmbLoss\n",
    "from recbole.model.init import xavier_normal_initialization\n",
    "from recbole.model.layers import BiGNNLayer, SparseDropout\n",
    "\n",
    "from logging import getLogger\n",
    "from recbole.quick_start import run_recbole\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.general_recommender import BPR\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "from recbole.utils import get_model, get_trainer\n",
    "from recbole.trainer import HyperTuning\n",
    "from recbole.quick_start import objective_function\n",
    "# import create_dataset from recbole\n",
    "from recbole.data import create_dataset\n",
    "\n",
    "from newmodel import NGCFpretrain,LightGCNpretrain, NGCFconcat, LightGCNconcat, NGCFpretrainMLP, LightGCNpretrainMLP\n",
    "\n",
    "def get_last_file(path):\n",
    "    files = glob.glob(path + '/*')\n",
    "    return max(files, key=os.path.getctime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create atomic files \n",
    "- sparse matrix (.npz) 파일 -> interaction (.inter), features (.itememb) 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset/csr_matrix/azuki.npz', './dataset/csr_matrix/bayc.npz', './dataset/csr_matrix/coolcats.npz', './dataset/csr_matrix/doodles.npz', './dataset/csr_matrix/meebits.npz']\n"
     ]
    }
   ],
   "source": [
    "# .npz files are in './dataset/csr_matrix/'\n",
    "\n",
    "npz_files = glob.glob('./dataset/csr_matrix/*.npz')\n",
    "print(npz_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder './dataset/collections'\n",
    "if not os.path.exists('./dataset/collections'):\n",
    "    os.makedirs('./dataset/collections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'dataset/collections/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_64885/1119743404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset/collections'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/collections/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/collections/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset/collections/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/anaconda3/envs/LEE_nft/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'dataset/collections/'"
     ]
    }
   ],
   "source": [
    "for npz in npz_files:\n",
    "    sparse_matrix = scipy.sparse.load_npz(npz)\n",
    "    coo_sparse_matrix = sparse_matrix.tocoo()\n",
    "\n",
    "    user = coo_sparse_matrix.row\n",
    "    item = coo_sparse_matrix.col\n",
    "    data = np.ones(shape=(len(user),), dtype=np.int32) # coo_sparse_matrix.data\n",
    "\n",
    "    f = open('./dataset/collections/' + npz[21:-4] + \".inter\", 'w')\n",
    "    f.write(\"user_id:token\\titem_id:token\\trating:float\\n\")\n",
    "    for i in range(len(user)):\n",
    "        f.write(\"%d\\t%d\\t%d\\n\"%(user[i],item[i],data[i]))\n",
    "    f.close()\n",
    "\n",
    "file_path = os.listdir('./dataset/collections')\n",
    "for name in file_path:\n",
    "    os.makedirs('dataset/collections/'+name[:-6])\n",
    "    shutil.move('dataset/collections/'+name, 'dataset/collections/'+name[:-6]+'/'+name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset/collections/azuki/azuki.inter', './dataset/collections/bayc/bayc.inter', './dataset/collections/coolcats/coolcats.inter', './dataset/collections/doodles/doodles.inter', './dataset/collections/meebits/meebits.inter']\n"
     ]
    }
   ],
   "source": [
    "# .inter files are in folders in './dataset/collections/*/'\n",
    "\n",
    "inter_files = glob.glob('./dataset/collections/*/*.inter')\n",
    "print(inter_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset names are in './dataset/collections/'\n",
    "\n",
    "DATASET_names = os.listdir('./dataset/collections/')\n",
    "DATASET_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .itememb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get collection names in folder './dataset/collections'\n",
    "collection_names = os.listdir('./dataset/collections')\n",
    "collection_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  azuki\n",
      "before:  (10000, 65)\n",
      "after:  (8386, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8386/8386 [00:07<00:00, 1195.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  bayc\n",
      "before:  (9983, 65)\n",
      "after:  (4008, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4008/4008 [00:03<00:00, 1251.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  coolcats\n",
      "before:  (9952, 65)\n",
      "after:  (4908, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4908/4908 [00:04<00:00, 1137.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  doodles\n",
      "before:  (9999, 65)\n",
      "after:  (7641, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7641/7641 [00:05<00:00, 1318.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  meebits\n",
      "before:  (12306, 65)\n",
      "after:  (4942, 65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4942/4942 [00:03<00:00, 1237.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for collection in collection_names:\n",
    "    \n",
    "    print('--- ', collection)\n",
    "    \n",
    "    # get csv file with '_image' in their name\n",
    "    img_file = pd.read_csv(f'./dataset/item_features/{collection}_image.csv')\n",
    "    print('before: ', img_file.shape)\n",
    "    \n",
    "    # 우리가 가진 기간 내 interaction에 등장하는 아이템만 남기기\n",
    "    # get meebits.inter file from './dataset/collections/meebits/'\n",
    "    inter = pd.read_csv(f'./dataset/collections/{collection}/{collection}.inter', sep='\\t')\n",
    "    # get unique values in column 'item_id:token'\n",
    "    token_ids = inter['item_id:token'].unique()\n",
    "    img_file = img_file[img_file['token_ID'].isin(token_ids)].reset_index(drop=True)\n",
    "    print('after: ', img_file.shape)\n",
    "    \n",
    "    # .itememb 저장하기\n",
    "    f = open(f\"./dataset/collections/{collection}/{collection}.itememb_img\", 'w')\n",
    "    f.write(\"iid_img:token\" + '\\t' + 'item_emb_img:float_seq' + '\\n')\n",
    "    for i in tqdm(range(len(img_file))):\n",
    "        # get token_id\n",
    "        token_id = img_file['token_ID'][i]\n",
    "        # get the rest of the features\n",
    "        features = img_file.iloc[i, 1:] # Series\n",
    "        # write\n",
    "        f.write(str(token_id) + '\\t')\n",
    "        for j in range(len(features)):\n",
    "            f.write(f\"{features[j].astype(np.float32)}\") \n",
    "            # if it is not the last iteration\n",
    "            if j != len(features) - 1:\n",
    "                f.write(' ')\n",
    "        f.write('\\n')    \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get collection names in folder './dataset/collections'\n",
    "collection_names = os.listdir('./dataset/collections')\n",
    "collection_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  azuki\n",
      "before:  (10000, 1801)\n",
      "after:  (8386, 1801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8386/8386 [01:45<00:00, 79.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  bayc\n",
      "before:  (10000, 1801)\n",
      "after:  (4025, 1801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4025/4025 [00:49<00:00, 81.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  coolcats\n",
      "before:  (9941, 1501)\n",
      "after:  (4903, 1501)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4903/4903 [00:53<00:00, 92.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  doodles\n",
      "before:  (10000, 1501)\n",
      "after:  (7642, 1501)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7642/7642 [01:27<00:00, 86.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  meebits\n",
      "before:  (20000, 1801)\n",
      "after:  (5702, 1801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5702/5702 [01:21<00:00, 70.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for collection in collection_names:\n",
    "    \n",
    "    print('--- ', collection)\n",
    "    \n",
    "    # get csv file with '_text' in their name\n",
    "    txt_file = pd.read_csv(f'./dataset/item_features/{collection}_text.csv')\n",
    "    print('before: ', txt_file.shape)\n",
    "    \n",
    "    # 우리가 가진 기간 내 interaction에 등장하는 아이템만 남기기\n",
    "    # get meebits.inter file from './dataset/collections/meebits/'\n",
    "    inter = pd.read_csv(f'./dataset/collections/{collection}/{collection}.inter', sep='\\t')\n",
    "    # get unique values in column 'item_id:token'\n",
    "    token_ids = inter['item_id:token'].unique()\n",
    "    txt_file = txt_file[txt_file['Token ID'].isin(token_ids)].reset_index(drop=True)\n",
    "    print('after: ', txt_file.shape)\n",
    "    \n",
    "    # .itememb 저장하기\n",
    "    f = open(f\"./dataset/collections/{collection}/{collection}.itememb_txt\", 'w')\n",
    "    f.write(\"iid_txt:token\" + '\\t' + 'item_emb_txt:float_seq' + '\\n')\n",
    "    for i in tqdm(range(len(txt_file))):\n",
    "        # get token_id\n",
    "        token_id = txt_file['Token ID'][i]\n",
    "        # get the rest of the features\n",
    "        features = txt_file.iloc[i, 1:] # Series\n",
    "        # write\n",
    "        f.write(str(token_id) + '\\t')\n",
    "        for j in range(len(features)):\n",
    "            f.write(f\"{features[j].astype(np.float32)}\") \n",
    "            # if it is not the last iteration\n",
    "            if j != len(features) - 1:\n",
    "                f.write(' ')\n",
    "        f.write('\\n')    \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get collection names in folder './dataset/collections'\n",
    "collection_names = os.listdir('./dataset/collections')\n",
    "collection_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  azuki\n",
      "before:  (8386, 2)\n",
      "after:  (8386, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8386/8386 [00:01<00:00, 4778.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  bayc\n",
      "before:  (4025, 2)\n",
      "after:  (4025, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4025/4025 [00:00<00:00, 4726.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  coolcats\n",
      "before:  (4908, 2)\n",
      "after:  (4908, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4908/4908 [00:01<00:00, 4777.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  doodles\n",
      "before:  (7642, 2)\n",
      "after:  (7642, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7642/7642 [00:02<00:00, 3524.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  meebits\n",
      "before:  (5702, 2)\n",
      "after:  (5702, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5702/5702 [00:01<00:00, 3950.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for collection in collection_names:\n",
    "    \n",
    "    print('--- ', collection)\n",
    "    \n",
    "    # get csv file with '_text' in their name\n",
    "    price_file = pd.read_csv(f'./dataset/item_features/{collection}_price.csv')\n",
    "    print('before: ', price_file.shape)\n",
    "    \n",
    "    # 우리가 가진 기간 내 interaction에 등장하는 아이템만 남기기\n",
    "    # get meebits.inter file from './dataset/collections/meebits/'\n",
    "    inter = pd.read_csv(f'./dataset/collections/{collection}/{collection}.inter', sep='\\t')\n",
    "    # get unique values in column 'item_id:token'\n",
    "    token_ids = inter['item_id:token'].unique()\n",
    "    price_file = price_file[price_file['TokenID'].isin(token_ids)].reset_index(drop=True)\n",
    "    print('after: ', price_file.shape)\n",
    "    \n",
    "    # .itememb 저장하기\n",
    "    f = open(f\"./dataset/collections/{collection}/{collection}.itememb_price\", 'w')\n",
    "    f.write(\"iid_price:token\" + '\\t' + 'item_emb_price:float_seq' + '\\n')\n",
    "    for i in tqdm(range(len(price_file))):\n",
    "        # get token_id\n",
    "        token_id = price_file['TokenID'][i]\n",
    "        # get the rest of the features\n",
    "        features = price_file.iloc[i, 1:] # Series\n",
    "        # write\n",
    "        f.write(str(token_id) + '\\t')\n",
    "        for j in range(len(features)):\n",
    "            f.write(f\"{features[j].astype(np.float32)}\") \n",
    "            # if it is not the last iteration\n",
    "            if j != len(features) - 1:\n",
    "                f.write(' ')\n",
    "        f.write('\\n')    \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "    \n",
    "    # environment\n",
    "    'seed': 0,\n",
    "    'reproducibility': True,\n",
    "    'data_path': 'dataset/collections/',\n",
    "    'checkpoint_dir': 'saved/',\n",
    "    'show_progress': True,\n",
    "    'save_dataset': False,\n",
    "    'log_wandb': False,\n",
    "    \n",
    "    # data\n",
    "    'field_separator': '\\t',\n",
    "    'seq_separator': ' ',\n",
    "    'USER_ID_FIELD': 'user_id',\n",
    "    'ITEM_ID_FIELD': 'item_id',\n",
    "    'RATING_FIELD': 'rating',\n",
    "    'item_inter_num_interval': '[0,inf)', \n",
    "    \n",
    "    # training\n",
    "    'epochs': 50,\n",
    "    'train_batch_size': 2048, # 2048\n",
    "    'learner': 'adam',\n",
    "    'learning_rate': 0.1, # 0.001\n",
    "    'train_neg_sample_args': {'distribution': 'popularity',\n",
    "                              'sample_num': 5,\n",
    "                              'dynamic': False,\n",
    "                              'candidate_num': 0},\n",
    "    'eval_step': 1,\n",
    "    'stopping_step': 15000000000000000000000000000000, # 15\n",
    "    'loss_decimal_place': 4,\n",
    "    \n",
    "    # evaluation\n",
    "    'eval_args': {'group_by': 'user',\n",
    "                  'order': 'RO',\n",
    "                  'split': {'RS':[8,1,1]},\n",
    "                  'mode': 'pop100'},\n",
    "    'metrics': ['Recall', 'MRR', 'NDCG', 'Hit', 'MAP', 'Precision', 'GAUC'],\n",
    "    'topk': [1, 2, 5, 10, 20, 50], \n",
    "    'valid_metric': 'MRR@1', # for early stopping\n",
    "    'eval_batch_size': 4096, # 4096\n",
    "    'metric_decimal_place': 4\n",
    "    \n",
    "}\n",
    "\n",
    "# convert parameter_dict to yaml file\n",
    "with open(r'config/fixed_config_baseline.yaml', 'w') as file:\n",
    "    documents = yaml.dump(parameter_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "topk가 valid_metric MRR@10보다 작으면 에러남 -> 근데 얘네들을 맞춰주기가 어려움 -> early stopping을 버리던지 config를 따로 만들어야 함 -> 그냥 early stopping을 버리자 -> valid data 용도는 그냥 best model 뽑기 위한 것만 \n",
    "\"\"\"\n",
    "\n",
    "# # K = [1, 2, 5, 10, 20, 50, 100] \n",
    " \n",
    "# parameter_dict['topk'] = 2\n",
    "# parameter_dict['valid_metric'] = 'MRR@2'\n",
    "# K = parameter_dict['topk']\n",
    "# with open(r'config/fixed_config_K{0}.yaml'.format(K), 'w') as file:\n",
    "#     documents = yaml.dump(parameter_dict, file)\n",
    "\n",
    "# parameter_dict['topk'] = 5\n",
    "# parameter_dict['valid_metric'] = 'MRR@5'\n",
    "# K = parameter_dict['topk']\n",
    "# with open(r'config/fixed_config_K{0}.yaml'.format(K), 'w') as file:\n",
    "#     documents = yaml.dump(parameter_dict, file)\n",
    "\n",
    "\n",
    "# # .yaml files are in './config/multi/'\n",
    "# yaml_files = glob.glob('./config/multi/*.yaml')\n",
    "# yaml_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('LEE_nft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb8a4f99a7745556126fde78a65badf1e54f4b0c39f818b3bd8bf36b8a405c65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
